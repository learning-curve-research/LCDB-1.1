{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({\"text.usetex\": True, \"font.family\": \"serif\", \"font.serif\": [\"Computer Modern Roman\"]})\n",
    "\n",
    "from utils import global_monotonicity_violation, global_convexity_violation, peaking_detection\n",
    "from meta_feature import learner_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Path.cwd() / '../dataset/LCDB11_ER_265_24.hdf5'\n",
    "dataset_CC18 = h5py.File(file_paths , 'r')['error rate'][...] \n",
    "# dataset_nofs, dataset_minmaxfs, dataset_standardfs \n",
    "datasets = [dataset_CC18[..., 0, 0], dataset_CC18[..., 1, 0], dataset_CC18[..., 2, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [00:11<00:00, 22.51it/s]\n",
      "100%|██████████| 265/265 [08:00<00:00,  1.81s/it]\n",
      "100%|██████████| 265/265 [00:11<00:00, 23.79it/s]\n",
      "100%|██████████| 265/265 [08:04<00:00,  1.83s/it]\n",
      "100%|██████████| 265/265 [00:17<00:00, 15.49it/s]\n",
      "100%|██████████| 265/265 [08:35<00:00,  1.95s/it]\n",
      "100%|██████████| 265/265 [00:11<00:00, 22.41it/s]\n",
      "100%|██████████| 265/265 [06:56<00:00,  1.57s/it]\n",
      "100%|██████████| 265/265 [00:11<00:00, 22.68it/s]\n",
      "100%|██████████| 265/265 [05:33<00:00,  1.26s/it]\n",
      "100%|██████████| 265/265 [00:07<00:00, 35.78it/s]\n",
      "100%|██████████| 265/265 [05:30<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for DATASET in datasets: \n",
    "    mono_matrix_y, _ = global_monotonicity_violation(DATASET, flat_filter = True)\n",
    "    conv_matrix, _, _, _ = global_convexity_violation(DATASET, flat_filter = True)\n",
    "    dipping_matrix_y, _ = global_monotonicity_violation(DATASET, flat_filter = True, dipping = True)\n",
    "    peak_matrix, _, _, _ = peaking_detection(DATASET, flat_filter = True)\n",
    "    results.append({\n",
    "        \"mono_matrix_y\": mono_matrix_y,\n",
    "        \"conv_matrix\": conv_matrix,\n",
    "        \"dipping_matrix_y\": dipping_matrix_y,\n",
    "        \"peak_matrix\": peak_matrix,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Ratio: 2.89%\n",
      "Flat: 11.04%\n",
      "Monotone (M): 77.03%\n",
      "Convex (C): 77.50%\n",
      "Well-behaved (M+C): 74.53%\n",
      "Monotonicity Violation: 9.04%\n",
      "Convexity Violation: 8.57%\n",
      "Peaking: 5.36%\n",
      "Dipping: 6.57%\n",
      "-----------------------\n",
      "Missing Ratio: 0.38%\n",
      "Flat: 10.49%\n",
      "Monotone (M): 79.25%\n",
      "Convex (C): 80.71%\n",
      "Well-behaved (M+C): 76.95%\n",
      "Monotonicity Violation: 9.89%\n",
      "Convexity Violation: 8.43%\n",
      "Peaking: 5.33%\n",
      "Dipping: 8.18%\n",
      "-----------------------\n",
      "Missing Ratio: 8.33%\n",
      "Flat: 8.00%\n",
      "Monotone (M): 74.81%\n",
      "Convex (C): 75.71%\n",
      "Well-behaved (M+C): 72.89%\n",
      "Monotonicity Violation: 8.85%\n",
      "Convexity Violation: 7.96%\n",
      "Peaking: 5.02%\n",
      "Dipping: 6.32%\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    mono_matrix_y = result[\"mono_matrix_y\"]\n",
    "    conv_matrix = result[\"conv_matrix\"]\n",
    "    missing = (np.isnan(mono_matrix_y).sum() / mono_matrix_y.size) * 100\n",
    "    print(f\"Missing Ratio: {missing:.2f}%\") \n",
    "\n",
    "    print(f\"Flat: {(np.sum(mono_matrix_y == -1) / mono_matrix_y.size) * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Monotone (M): {(np.sum(mono_matrix_y == 0) / mono_matrix_y.size) * 100:.2f}%\")\n",
    "    print(f\"Convex (C): {(np.sum(conv_matrix == 0) / conv_matrix.size) * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Well-behaved (M+C): {(np.sum((conv_matrix == 0) & (mono_matrix_y == 0)) / mono_matrix_y.size) * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Monotonicity Violation: {(np.sum(mono_matrix_y > 0) / mono_matrix_y.size) * 100:.2f}%\")\n",
    "    print(f\"Convexity Violation: {(np.sum(conv_matrix > 0) / conv_matrix.size) * 100:.2f}%\")\n",
    "\n",
    "    dipping_matrix_y = result[\"dipping_matrix_y\"]\n",
    "    peak_matrix = result[\"peak_matrix\"]\n",
    "\n",
    "    print(f\"Peaking: {(np.sum(peak_matrix > 0) / peak_matrix.size) * 100:.2f}%\")\n",
    "    print(f\"Dipping: {(np.sum(dipping_matrix_y > 0) / dipping_matrix_y.size) * 100:.2f}%\")\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset 1 ===\n",
      "Learner SVM_Linear: Missing=0.38%, Flat=3.40%, Monotone=93.21%, Convex=92.83%, Well-behaved=91.70%, Peaking=1.51%, Dipping=2.64%\n",
      "Learner SVM_Poly: Missing=0.38%, Flat=17.36%, Monotone=80.75%, Convex=79.62%, Well-behaved=78.49%, Peaking=0.38%, Dipping=1.89%\n",
      "Learner SVM_RBF: Missing=0.38%, Flat=18.49%, Monotone=80.00%, Convex=73.58%, Well-behaved=73.21%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner SVM_Sigmoid: Missing=0.38%, Flat=19.25%, Monotone=36.98%, Convex=49.81%, Well-behaved=32.45%, Peaking=23.40%, Dipping=42.26%\n",
      "Learner Decision Tree: Missing=0.38%, Flat=4.53%, Monotone=94.72%, Convex=94.34%, Well-behaved=94.34%, Peaking=0.38%, Dipping=0.75%\n",
      "Learner ExtraTree: Missing=0.38%, Flat=3.77%, Monotone=94.72%, Convex=95.85%, Well-behaved=94.72%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner LogisticRegression: Missing=0.38%, Flat=6.79%, Monotone=91.32%, Convex=89.06%, Well-behaved=88.68%, Peaking=1.13%, Dipping=1.51%\n",
      "Learner PassiveAggressive: Missing=0.38%, Flat=5.66%, Monotone=86.42%, Convex=90.19%, Well-behaved=85.66%, Peaking=1.89%, Dipping=3.40%\n",
      "Learner Perceptron: Missing=0.38%, Flat=3.02%, Monotone=94.34%, Convex=95.47%, Well-behaved=93.96%, Peaking=0.75%, Dipping=1.51%\n",
      "Learner RidgeClassifier: Missing=0.38%, Flat=7.17%, Monotone=78.11%, Convex=77.74%, Well-behaved=76.23%, Peaking=10.94%, Dipping=4.91%\n",
      "Learner SGDClassifier: Missing=0.38%, Flat=2.26%, Monotone=95.09%, Convex=96.98%, Well-behaved=95.09%, Peaking=0.38%, Dipping=2.26%\n",
      "Learner MLP: Missing=0.38%, Flat=4.91%, Monotone=74.34%, Convex=73.58%, Well-behaved=67.55%, Peaking=9.81%, Dipping=3.77%\n",
      "Learner LDA: Missing=0.38%, Flat=3.77%, Monotone=63.77%, Convex=63.02%, Well-behaved=58.87%, Peaking=24.53%, Dipping=6.42%\n",
      "Learner QDA: Missing=0.38%, Flat=3.77%, Monotone=63.02%, Convex=60.00%, Well-behaved=51.32%, Peaking=19.62%, Dipping=26.79%\n",
      "Learner BernoulliNB: Missing=0.38%, Flat=26.42%, Monotone=66.79%, Convex=62.64%, Well-behaved=60.38%, Peaking=4.91%, Dipping=5.66%\n",
      "Learner MultinomialNB: Missing=30.57%, Flat=9.06%, Monotone=56.98%, Convex=55.09%, Well-behaved=54.72%, Peaking=3.02%, Dipping=4.53%\n",
      "Learner ComplementNB: Missing=30.57%, Flat=8.30%, Monotone=55.09%, Convex=56.23%, Well-behaved=54.34%, Peaking=4.15%, Dipping=6.79%\n",
      "Learner GaussianNB: Missing=0.38%, Flat=4.53%, Monotone=73.21%, Convex=80.38%, Well-behaved=71.32%, Peaking=12.83%, Dipping=24.53%\n",
      "Learner KNN: Missing=0.38%, Flat=10.94%, Monotone=87.17%, Convex=87.55%, Well-behaved=86.79%, Peaking=0.75%, Dipping=2.26%\n",
      "Learner NearestCentroid: Missing=0.38%, Flat=10.94%, Monotone=81.51%, Convex=84.15%, Well-behaved=81.13%, Peaking=4.15%, Dipping=11.32%\n",
      "Learner ens.ExtraTrees: Missing=0.38%, Flat=9.06%, Monotone=88.68%, Convex=89.06%, Well-behaved=87.92%, Peaking=0.75%, Dipping=1.89%\n",
      "Learner ens.RandomForest: Missing=0.38%, Flat=9.06%, Monotone=89.06%, Convex=89.43%, Well-behaved=88.30%, Peaking=0.38%, Dipping=1.13%\n",
      "Learner ens.GradientBoosting: Missing=0.38%, Flat=3.40%, Monotone=95.09%, Convex=96.23%, Well-behaved=95.09%, Peaking=0.00%, Dipping=0.75%\n",
      "Learner DummyClassifier: Missing=0.38%, Flat=69.06%, Monotone=28.30%, Convex=27.17%, Well-behaved=26.42%, Peaking=3.02%, Dipping=0.00%\n",
      "\n",
      "\n",
      "=== Dataset 2 ===\n",
      "Learner SVM_Linear: Missing=0.38%, Flat=7.92%, Monotone=89.81%, Convex=90.19%, Well-behaved=89.43%, Peaking=0.38%, Dipping=0.75%\n",
      "Learner SVM_Poly: Missing=0.38%, Flat=6.04%, Monotone=92.83%, Convex=91.70%, Well-behaved=90.94%, Peaking=0.00%, Dipping=1.13%\n",
      "Learner SVM_RBF: Missing=0.38%, Flat=13.58%, Monotone=84.91%, Convex=81.13%, Well-behaved=80.38%, Peaking=0.00%, Dipping=0.75%\n",
      "Learner SVM_Sigmoid: Missing=0.38%, Flat=20.00%, Monotone=39.62%, Convex=53.21%, Well-behaved=34.34%, Peaking=19.25%, Dipping=40.75%\n",
      "Learner Decision Tree: Missing=0.38%, Flat=2.26%, Monotone=96.60%, Convex=96.60%, Well-behaved=96.23%, Peaking=0.38%, Dipping=0.75%\n",
      "Learner ExtraTree: Missing=0.38%, Flat=2.26%, Monotone=96.23%, Convex=97.36%, Well-behaved=96.23%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner LogisticRegression: Missing=0.38%, Flat=11.32%, Monotone=86.79%, Convex=87.17%, Well-behaved=86.79%, Peaking=1.13%, Dipping=1.89%\n",
      "Learner PassiveAggressive: Missing=0.38%, Flat=5.28%, Monotone=92.08%, Convex=93.96%, Well-behaved=92.08%, Peaking=0.38%, Dipping=1.89%\n",
      "Learner Perceptron: Missing=0.38%, Flat=1.89%, Monotone=96.23%, Convex=97.36%, Well-behaved=96.23%, Peaking=0.38%, Dipping=1.13%\n",
      "Learner RidgeClassifier: Missing=0.38%, Flat=10.94%, Monotone=86.79%, Convex=85.28%, Well-behaved=85.28%, Peaking=1.51%, Dipping=1.51%\n",
      "Learner SGDClassifier: Missing=0.38%, Flat=3.02%, Monotone=95.47%, Convex=95.85%, Well-behaved=95.47%, Peaking=0.75%, Dipping=1.89%\n",
      "Learner MLP: Missing=0.38%, Flat=7.55%, Monotone=74.34%, Convex=69.43%, Well-behaved=66.04%, Peaking=6.79%, Dipping=1.51%\n",
      "Learner LDA: Missing=0.38%, Flat=4.15%, Monotone=63.40%, Convex=62.64%, Well-behaved=58.49%, Peaking=24.53%, Dipping=6.42%\n",
      "Learner QDA: Missing=0.38%, Flat=3.40%, Monotone=61.89%, Convex=56.60%, Well-behaved=48.68%, Peaking=21.13%, Dipping=29.81%\n",
      "Learner BernoulliNB: Missing=0.38%, Flat=21.89%, Monotone=64.15%, Convex=67.92%, Well-behaved=59.62%, Peaking=4.91%, Dipping=14.72%\n",
      "Learner MultinomialNB: Missing=0.38%, Flat=14.34%, Monotone=72.83%, Convex=77.74%, Well-behaved=69.81%, Peaking=4.15%, Dipping=16.98%\n",
      "Learner ComplementNB: Missing=0.38%, Flat=5.28%, Monotone=76.98%, Convex=83.40%, Well-behaved=76.23%, Peaking=10.19%, Dipping=17.74%\n",
      "Learner GaussianNB: Missing=0.38%, Flat=4.53%, Monotone=68.30%, Convex=75.85%, Well-behaved=66.79%, Peaking=17.36%, Dipping=25.66%\n",
      "Learner KNN: Missing=0.38%, Flat=10.19%, Monotone=87.92%, Convex=87.55%, Well-behaved=86.79%, Peaking=0.75%, Dipping=1.13%\n",
      "Learner NearestCentroid: Missing=0.38%, Flat=6.79%, Monotone=72.08%, Convex=83.40%, Well-behaved=72.08%, Peaking=9.43%, Dipping=26.04%\n",
      "Learner ens.ExtraTrees: Missing=0.38%, Flat=8.30%, Monotone=89.43%, Convex=89.81%, Well-behaved=88.68%, Peaking=0.75%, Dipping=1.89%\n",
      "Learner ens.RandomForest: Missing=0.38%, Flat=8.68%, Monotone=89.43%, Convex=89.43%, Well-behaved=88.68%, Peaking=0.75%, Dipping=0.75%\n",
      "Learner ens.GradientBoosting: Missing=0.38%, Flat=3.02%, Monotone=95.47%, Convex=96.23%, Well-behaved=95.09%, Peaking=0.00%, Dipping=0.75%\n",
      "Learner DummyClassifier: Missing=0.38%, Flat=69.06%, Monotone=28.30%, Convex=27.17%, Well-behaved=26.42%, Peaking=3.02%, Dipping=0.00%\n",
      "\n",
      "\n",
      "=== Dataset 3 ===\n",
      "Learner SVM_Linear: Missing=0.00%, Flat=2.26%, Monotone=94.34%, Convex=93.96%, Well-behaved=92.83%, Peaking=1.51%, Dipping=2.26%\n",
      "Learner SVM_Poly: Missing=0.00%, Flat=12.83%, Monotone=85.66%, Convex=83.77%, Well-behaved=83.40%, Peaking=0.38%, Dipping=0.38%\n",
      "Learner SVM_RBF: Missing=0.00%, Flat=12.45%, Monotone=85.28%, Convex=83.77%, Well-behaved=82.64%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner SVM_Sigmoid: Missing=0.00%, Flat=11.70%, Monotone=49.81%, Convex=64.91%, Well-behaved=47.55%, Peaking=20.75%, Dipping=40.00%\n",
      "Learner Decision Tree: Missing=0.00%, Flat=3.02%, Monotone=96.23%, Convex=96.23%, Well-behaved=95.85%, Peaking=0.38%, Dipping=0.75%\n",
      "Learner ExtraTree: Missing=0.00%, Flat=2.64%, Monotone=96.23%, Convex=97.36%, Well-behaved=96.23%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner LogisticRegression: Missing=0.00%, Flat=7.92%, Monotone=89.81%, Convex=89.06%, Well-behaved=88.68%, Peaking=1.89%, Dipping=0.75%\n",
      "Learner PassiveAggressive: Missing=0.00%, Flat=1.89%, Monotone=93.96%, Convex=96.60%, Well-behaved=93.58%, Peaking=0.75%, Dipping=3.02%\n",
      "Learner Perceptron: Missing=0.00%, Flat=1.13%, Monotone=95.85%, Convex=96.98%, Well-behaved=95.85%, Peaking=1.51%, Dipping=1.89%\n",
      "Learner RidgeClassifier: Missing=0.00%, Flat=6.04%, Monotone=79.25%, Convex=78.49%, Well-behaved=76.60%, Peaking=10.19%, Dipping=2.64%\n",
      "Learner SGDClassifier: Missing=0.00%, Flat=0.75%, Monotone=95.85%, Convex=96.23%, Well-behaved=95.09%, Peaking=1.89%, Dipping=1.89%\n",
      "Learner MLP: Missing=0.00%, Flat=3.02%, Monotone=87.92%, Convex=83.40%, Well-behaved=80.75%, Peaking=3.02%, Dipping=1.89%\n",
      "Learner LDA: Missing=0.00%, Flat=3.02%, Monotone=64.91%, Convex=63.40%, Well-behaved=59.25%, Peaking=23.40%, Dipping=7.17%\n",
      "Learner QDA: Missing=0.00%, Flat=1.89%, Monotone=60.00%, Convex=58.49%, Well-behaved=50.57%, Peaking=21.13%, Dipping=33.58%\n",
      "Learner BernoulliNB: Missing=0.00%, Flat=9.06%, Monotone=83.77%, Convex=83.77%, Well-behaved=80.38%, Peaking=3.77%, Dipping=7.55%\n",
      "Learner MultinomialNB: Missing=100.00%, Flat=0.00%, Monotone=0.00%, Convex=0.00%, Well-behaved=0.00%, Peaking=0.00%, Dipping=0.00%\n",
      "Learner ComplementNB: Missing=100.00%, Flat=0.00%, Monotone=0.00%, Convex=0.00%, Well-behaved=0.00%, Peaking=0.00%, Dipping=0.00%\n",
      "Learner GaussianNB: Missing=0.00%, Flat=3.40%, Monotone=69.06%, Convex=75.85%, Well-behaved=67.17%, Peaking=17.74%, Dipping=26.79%\n",
      "Learner KNN: Missing=0.00%, Flat=10.94%, Monotone=84.15%, Convex=85.66%, Well-behaved=83.02%, Peaking=1.13%, Dipping=3.02%\n",
      "Learner NearestCentroid: Missing=0.00%, Flat=8.68%, Monotone=79.25%, Convex=84.91%, Well-behaved=79.25%, Peaking=6.42%, Dipping=14.34%\n",
      "Learner ens.ExtraTrees: Missing=0.00%, Flat=8.30%, Monotone=89.81%, Convex=90.19%, Well-behaved=89.06%, Peaking=0.75%, Dipping=1.89%\n",
      "Learner ens.RandomForest: Missing=0.00%, Flat=8.30%, Monotone=90.19%, Convex=90.19%, Well-behaved=89.43%, Peaking=0.75%, Dipping=0.75%\n",
      "Learner ens.GradientBoosting: Missing=0.00%, Flat=3.02%, Monotone=96.23%, Convex=96.98%, Well-behaved=96.23%, Peaking=0.00%, Dipping=0.38%\n",
      "Learner DummyClassifier: Missing=0.00%, Flat=69.81%, Monotone=27.92%, Convex=26.79%, Well-behaved=26.04%, Peaking=3.02%, Dipping=0.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_learner_stats = []\n",
    "\n",
    "for result in results:\n",
    "    mono_matrix_y = result[\"mono_matrix_y\"]\n",
    "    conv_matrix = result[\"conv_matrix\"]\n",
    "    dipping_matrix_y = result[\"dipping_matrix_y\"]\n",
    "    peak_matrix = result[\"peak_matrix\"]\n",
    "\n",
    "    learner_stats = [{\"learner\": learner_zoo[i]} for i in range(24)]\n",
    "\n",
    "    for i in range(24):\n",
    "        mono_y_learner = mono_matrix_y[i, :]\n",
    "        conv_learner = conv_matrix[i, :]\n",
    "        dipping_y_learner = dipping_matrix_y[i, :]\n",
    "        peak_learner = peak_matrix[i, :]\n",
    "\n",
    "        missing_learner = (np.isnan(mono_y_learner).sum() / mono_y_learner.size) * 100\n",
    "        flat_percentage_learner = (np.sum(mono_y_learner == -1) / mono_y_learner.size) * 100\n",
    "        mono_viola_percentage_learner = (np.sum(mono_y_learner == 0) / mono_y_learner.size) * 100\n",
    "        conv_viola_percentage_learner = (np.sum(conv_learner == 0) / conv_learner.size) * 100\n",
    "        both_no_viola_percentage_learner = (np.sum((conv_learner == 0) & (mono_y_learner == 0)) / mono_y_learner.size) * 100\n",
    "        peaking_percentage_learner = (np.sum(peak_learner > 0) / peak_learner.size) * 100\n",
    "        dipping_percentage_learner = (np.sum(dipping_y_learner > 0) / dipping_y_learner.size) * 100\n",
    "\n",
    "        learner_stats[i].update({\n",
    "            \"missing\": missing_learner,\n",
    "            \"flat\": flat_percentage_learner,\n",
    "            \"monotone\": mono_viola_percentage_learner,\n",
    "            \"convex\": conv_viola_percentage_learner,\n",
    "            \"well_behaved\": both_no_viola_percentage_learner,\n",
    "            \"peaking\": peaking_percentage_learner,\n",
    "            \"dipping\": dipping_percentage_learner,\n",
    "        })\n",
    "\n",
    "    all_learner_stats.append({\n",
    "        \"learner_stats\": learner_stats\n",
    "    })\n",
    "\n",
    "for dataset_idx, dataset_result in enumerate(all_learner_stats):\n",
    "    print(f\"=== Dataset {dataset_idx + 1} ===\")\n",
    "    for learner_stat in dataset_result[\"learner_stats\"]:\n",
    "        print(\n",
    "            f\"Learner {learner_stat['learner']}: \"\n",
    "            f\"Missing={learner_stat['missing']:.2f}%, \"\n",
    "            f\"Flat={learner_stat['flat']:.2f}%, \"\n",
    "            f\"Monotone={learner_stat['monotone']:.2f}%, \"\n",
    "            f\"Convex={learner_stat['convex']:.2f}%, \"\n",
    "            f\"Well-behaved={learner_stat['well_behaved']:.2f}%, \"\n",
    "            f\"Peaking={learner_stat['peaking']:.2f}%, \"\n",
    "            f\"Dipping={learner_stat['dipping']:.2f}%\"\n",
    "        )\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{sidewaystable}\n",
      "\\caption{Statistics of each learner in LCDB 1.1 (no Data Leakage version)}\n",
      "\\label{tab:learner_stats}\n",
      "\\resizebox{\\textwidth}{!}{ \n",
      "\\begin{tabular}{lccccccccccccccccccccc}\n",
      "\\toprule\n",
      "\\multirow{2}{*}{Learner} \n",
      "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) no FS}               \n",
      "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) min-max FS}           \n",
      "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) standardization FS}       \n",
      "\\\\\n",
      "\\cmidrule(lr){2-8} \\cmidrule(lr){9-15} \\cmidrule(lr){16-22}\n",
      "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
      "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
      "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
      "\\\\\n",
      "\\midrule\n",
      "SVM_Linear & 0.38 & 3.40 & 93.21 & 92.83 & 91.70 & 1.51 & 2.64 & 0.38 & 7.92 & 89.81 & 90.19 & 89.43 & 0.38 & 0.75 & 0.00 & 2.26 & 94.34 & 93.96 & 92.83 & 1.51 & 2.26 \\\\\n",
      "SVM_Poly & 0.38 & 17.36 & 80.75 & 79.62 & 78.49 & 0.38 & 1.89 & 0.38 & 6.04 & 92.83 & 91.70 & 90.94 & 0.00 & 1.13 & 0.00 & 12.83 & 85.66 & 83.77 & 83.40 & 0.38 & 0.38 \\\\\n",
      "SVM_RBF & 0.38 & 18.49 & 80.00 & 73.58 & 73.21 & 0.00 & 0.38 & 0.38 & 13.58 & 84.91 & 81.13 & 80.38 & 0.00 & 0.75 & 0.00 & 12.45 & 85.28 & 83.77 & 82.64 & 0.00 & 0.38 \\\\\n",
      "SVM_Sigmoid & 0.38 & 19.25 & 36.98 & 49.81 & 32.45 & 23.40 & 42.26 & 0.38 & 20.00 & 39.62 & 53.21 & 34.34 & 19.25 & 40.75 & 0.00 & 11.70 & 49.81 & 64.91 & 47.55 & 20.75 & 40.00 \\\\\n",
      "Decision Tree & 0.38 & 4.53 & 94.72 & 94.34 & 94.34 & 0.38 & 0.75 & 0.38 & 2.26 & 96.60 & 96.60 & 96.23 & 0.38 & 0.75 & 0.00 & 3.02 & 96.23 & 96.23 & 95.85 & 0.38 & 0.75 \\\\\n",
      "ExtraTree & 0.38 & 3.77 & 94.72 & 95.85 & 94.72 & 0.00 & 0.38 & 0.38 & 2.26 & 96.23 & 97.36 & 96.23 & 0.00 & 0.38 & 0.00 & 2.64 & 96.23 & 97.36 & 96.23 & 0.00 & 0.38 \\\\\n",
      "LogisticRegression & 0.38 & 6.79 & 91.32 & 89.06 & 88.68 & 1.13 & 1.51 & 0.38 & 11.32 & 86.79 & 87.17 & 86.79 & 1.13 & 1.89 & 0.00 & 7.92 & 89.81 & 89.06 & 88.68 & 1.89 & 0.75 \\\\\n",
      "PassiveAggressive & 0.38 & 5.66 & 86.42 & 90.19 & 85.66 & 1.89 & 3.40 & 0.38 & 5.28 & 92.08 & 93.96 & 92.08 & 0.38 & 1.89 & 0.00 & 1.89 & 93.96 & 96.60 & 93.58 & 0.75 & 3.02 \\\\\n",
      "Perceptron & 0.38 & 3.02 & 94.34 & 95.47 & 93.96 & 0.75 & 1.51 & 0.38 & 1.89 & 96.23 & 97.36 & 96.23 & 0.38 & 1.13 & 0.00 & 1.13 & 95.85 & 96.98 & 95.85 & 1.51 & 1.89 \\\\\n",
      "RidgeClassifier & 0.38 & 7.17 & 78.11 & 77.74 & 76.23 & 10.94 & 4.91 & 0.38 & 10.94 & 86.79 & 85.28 & 85.28 & 1.51 & 1.51 & 0.00 & 6.04 & 79.25 & 78.49 & 76.60 & 10.19 & 2.64 \\\\\n",
      "SGDClassifier & 0.38 & 2.26 & 95.09 & 96.98 & 95.09 & 0.38 & 2.26 & 0.38 & 3.02 & 95.47 & 95.85 & 95.47 & 0.75 & 1.89 & 0.00 & 0.75 & 95.85 & 96.23 & 95.09 & 1.89 & 1.89 \\\\\n",
      "MLP & 0.38 & 4.91 & 74.34 & 73.58 & 67.55 & 9.81 & 3.77 & 0.38 & 7.55 & 74.34 & 69.43 & 66.04 & 6.79 & 1.51 & 0.00 & 3.02 & 87.92 & 83.40 & 80.75 & 3.02 & 1.89 \\\\\n",
      "LDA & 0.38 & 3.77 & 63.77 & 63.02 & 58.87 & 24.53 & 6.42 & 0.38 & 4.15 & 63.40 & 62.64 & 58.49 & 24.53 & 6.42 & 0.00 & 3.02 & 64.91 & 63.40 & 59.25 & 23.40 & 7.17 \\\\\n",
      "QDA & 0.38 & 3.77 & 63.02 & 60.00 & 51.32 & 19.62 & 26.79 & 0.38 & 3.40 & 61.89 & 56.60 & 48.68 & 21.13 & 29.81 & 0.00 & 1.89 & 60.00 & 58.49 & 50.57 & 21.13 & 33.58 \\\\\n",
      "BernoulliNB & 0.38 & 26.42 & 66.79 & 62.64 & 60.38 & 4.91 & 5.66 & 0.38 & 21.89 & 64.15 & 67.92 & 59.62 & 4.91 & 14.72 & 0.00 & 9.06 & 83.77 & 83.77 & 80.38 & 3.77 & 7.55 \\\\\n",
      "MultinomialNB & 30.57 & 9.06 & 56.98 & 55.09 & 54.72 & 3.02 & 4.53 & 0.38 & 14.34 & 72.83 & 77.74 & 69.81 & 4.15 & 16.98 & 100.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "ComplementNB & 30.57 & 8.30 & 55.09 & 56.23 & 54.34 & 4.15 & 6.79 & 0.38 & 5.28 & 76.98 & 83.40 & 76.23 & 10.19 & 17.74 & 100.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "GaussianNB & 0.38 & 4.53 & 73.21 & 80.38 & 71.32 & 12.83 & 24.53 & 0.38 & 4.53 & 68.30 & 75.85 & 66.79 & 17.36 & 25.66 & 0.00 & 3.40 & 69.06 & 75.85 & 67.17 & 17.74 & 26.79 \\\\\n",
      "KNN & 0.38 & 10.94 & 87.17 & 87.55 & 86.79 & 0.75 & 2.26 & 0.38 & 10.19 & 87.92 & 87.55 & 86.79 & 0.75 & 1.13 & 0.00 & 10.94 & 84.15 & 85.66 & 83.02 & 1.13 & 3.02 \\\\\n",
      "NearestCentroid & 0.38 & 10.94 & 81.51 & 84.15 & 81.13 & 4.15 & 11.32 & 0.38 & 6.79 & 72.08 & 83.40 & 72.08 & 9.43 & 26.04 & 0.00 & 8.68 & 79.25 & 84.91 & 79.25 & 6.42 & 14.34 \\\\\n",
      "ens.ExtraTrees & 0.38 & 9.06 & 88.68 & 89.06 & 87.92 & 0.75 & 1.89 & 0.38 & 8.30 & 89.43 & 89.81 & 88.68 & 0.75 & 1.89 & 0.00 & 8.30 & 89.81 & 90.19 & 89.06 & 0.75 & 1.89 \\\\\n",
      "ens.RandomForest & 0.38 & 9.06 & 89.06 & 89.43 & 88.30 & 0.38 & 1.13 & 0.38 & 8.68 & 89.43 & 89.43 & 88.68 & 0.75 & 0.75 & 0.00 & 8.30 & 90.19 & 90.19 & 89.43 & 0.75 & 0.75 \\\\\n",
      "ens.GradientBoosting & 0.38 & 3.40 & 95.09 & 96.23 & 95.09 & 0.00 & 0.75 & 0.38 & 3.02 & 95.47 & 96.23 & 95.09 & 0.00 & 0.75 & 0.00 & 3.02 & 96.23 & 96.98 & 96.23 & 0.00 & 0.38 \\\\\n",
      "DummyClassifier & 0.38 & 69.06 & 28.30 & 27.17 & 26.42 & 3.02 & 0.00 & 0.38 & 69.06 & 28.30 & 27.17 & 26.42 & 3.02 & 0.00 & 0.00 & 69.81 & 27.92 & 26.79 & 26.04 & 3.02 & 0.00 \\\\\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "} \n",
      "\\end{sidewaystable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_data = []\n",
    "\n",
    "for i in range(24):  \n",
    "    row = [learner_zoo[i]]  \n",
    "    for dataset_result in all_learner_stats:\n",
    "        learner_stat = dataset_result[\"learner_stats\"][i]\n",
    "        row.extend([\n",
    "            f\"{learner_stat['missing']:.2f}\",\n",
    "            f\"{learner_stat['flat']:.2f}\",\n",
    "            f\"{learner_stat['monotone']:.2f}\",\n",
    "            f\"{learner_stat['convex']:.2f}\",\n",
    "            f\"{learner_stat['well_behaved']:.2f}\",\n",
    "            f\"{learner_stat['peaking']:.2f}\",\n",
    "            f\"{learner_stat['dipping']:.2f}\",\n",
    "        ])\n",
    "    table_data.append(row)\n",
    "\n",
    "\n",
    "latex_code = r\"\"\"\n",
    "\\begin{sidewaystable}\n",
    "\\caption{Statistics of each learner in LCDB 1.1 (no Data Leakage version)}\n",
    "\\label{tab:learner_stats}\n",
    "\\resizebox{\\textwidth}{!}{ \n",
    "\\begin{tabular}{lccccccccccccccccccccc}\n",
    "\\toprule\n",
    "\\multirow{2}{*}{Learner} \n",
    "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) no FS}               \n",
    "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) min-max FS}           \n",
    "& \\multicolumn{7}{c}{LCDB 1.1 FULL (265) standardization FS}       \n",
    "\\\\\n",
    "\\cmidrule(lr){2-8} \\cmidrule(lr){9-15} \\cmidrule(lr){16-22}\n",
    "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
    "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
    "& Missing & Flat & Monotone & Convex & Mono \\& Conv & Peaking & Dipping\n",
    "\\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for row in table_data:\n",
    "    latex_code += \" & \".join(row) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "\n",
    "latex_code += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "} \n",
    "\\end{sidewaystable}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(latex_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
